"""
Attention layers
"""


import torch
import torch.nn as nn
import torch.nn.functional as F


#TODO: Add custom variant of multi-head attenton, since Torch's implementation is just bad. 
